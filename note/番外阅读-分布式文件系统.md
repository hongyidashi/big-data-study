# 番外阅读-分布式文件系统
## 目录  
1. [概述](#概述)
2. [对分布式文件系统的要求](#对分布式文件系统的要求)
3. [架构模型](#架构模型)
4. [持久化](#持久化)
5. [伸缩性](#伸缩性)
6. [高可用性](#高可用性)
7. [性能优化和缓存一致性](#性能优化和缓存一致性)

## <span id="概述">概述</span>
分布式文件系统是分布式领域的一个基础应用，其中最著名的毫无疑问是 HDFS/GFS。
分布式文件系统并非只有 HDFS/GFS 这一种形态，在它之外，还有其他形态各异、各有千秋的产品形态，
对它们的了解，也对扩展我们的视野有所俾益。

## <span id="过去的样子">过去的样子</span>
分布式文件系统很早就已经出现了，以 Sun 在 1984 年开发的“Network File System (NFS)”为代表，
那时候解决的主要问题，是网络形态的磁盘，把磁盘从主机中独立出来。这样不仅可以获得**更大的容量**，而且还可以**随时切换主机**，
还可以实现**数据共享、备份、容灾等**，因为数据是电脑中最重要的资产。NFS 的数据通信图如下：  
![NFS数据通信图](https://images2017.cnblogs.com/blog/1054216/201709/1054216-20170901171106405-2068615273.jpg)

部署在主机上的客户端，通过 TCP/IP 协议把文件命令转发到远程文件 Server 上执行，整个过程对主机用户透明（即用户无感知）。

到了互联网时代，流量和数据快速增长，分布式文件系统所要解决的主要场景变了，开始需要非常大的磁盘空间，这在磁盘体系上垂直扩容是无法达到的，
必须要分布式，同时分布式架构下，主机都是可靠性不是非常好的普通服务器，因此容错、高可用、持久化、伸缩性等指标，就成为必须要考量的特性。

总结：  
最初使用分布式文件系统是为了**获得更大的容量和可随时切换主机（即主机切换不影响数据），还为了便于备份、容灾等**；  
现在使用分布式文件系统是因为**数据量变多，单纯的增加磁盘空间已经无法满足使用需求**，并且由于分布式架构的**主机性能都比较一般，
所以必须要考虑容错、高可用、持久化、伸缩性等指标**。

## <span id="对分布式文件系统的要求">对分布式文件系统的要求</span>
对一个分布式文件系统而言，有一些特性是必须要满足的，否则就无法有竞争力。主要如下：

1. 应该符合 POSIX 的文件接口标准，使该系统易于使用，同时对于用户的遗留系统也无需改造；
2. 对用户透明，即能够像使用本地文件系统那样直接使用；
3. 持久化，保证数据不会丢失；
4. 具有伸缩性，当数据压力逐渐增长时能顺利扩容；
5. 具有可靠的安全机制，保证数据安全；
6. 数据一致性，只要文件内容不发生变化，什么时候去读，得到的内容应该都是一样的。

除此之外，还有些特性是分布式加分项，具体如下：

1. 支持的空间越大越好；~~（废话）~~
2. 支持的并发访问请求越多越好；~~（废话）~~
3. 性能越快越好；~~（废话）~~
4. 硬件资源的利用率越高越合理，就越好。~~（废话）~~

~~**都说了这是加分项啊！喂！**~~

## <span id="架构模型">架构模型</span>

从业务模型和逻辑架构上，分布式文件系统需要这几类组件：

- **存储组件**：负责存储文件数据，它要保证文件的持久化、副本间数据一致、数据块的分配 / 合并等等；
- **管理组件**：负责 meta 信息，即文件数据的元信息，包括文件存放在哪台服务器上、文件大小、权限等，除此之外，
还要负责对 存储组件 的管理，包括存储组件所在的服务器是否正常存活、是否需要数据迁移等；
- **接口组件**：提供接口服务给应用使用，形态包括 SDK(Java/C/C++ 等)、CLI 命令行终端、以及支持 FUSE 挂载机制。

而在部署架构上，有着“**中心化**”和“**无中心化**”两种路线分歧，即是否把“管理组件”作为分布式文件系统的中心管理节点。两种路线都有很优秀的产品。

**有中心节点**  
以 GFS 为代表（跟 HDFS 差不多），中心节点负责文件定位、维护文件 meta 信息、故障检测、数据迁移等管理控制的职能，下图是 GFS 的架构图：  
![GFS架构图](https://img-blog.csdnimg.cn/20190610211005420.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RyZWFtdnlwcw==,size_16,color_FFFFFF,t_70)

该图中 GFS master 即为 GFS 的中心节点，GF chunk server 为 GFS 的存储节点。其操作路径如下：
1. Client 向中心节点请求“查询某个文件的某部分数据”；
2. 中心节点返回文件所在的位置 (哪台 chunk server 上的哪个文件) 以及字节区间信息；
3. Client 根据中心节点返回的信息，向对应的 chunk server 直接发送数据读取的请求；
4. chunk server 返回数据。

在这种方案里，一般中心节点并不参与真正的数据读写，而是将文件 meta 信息返回给 Client 之后，即由 Client 与数据节点直接通信，
其主要目的是**降低中心节点的负载**，防止其成为瓶颈。这种方案得到了广泛应用，因为**中心节点易控制、功能强大**。

>降低中心节点的负载  
>关于这个 降低中心节点的负载 是指 **中心节点不参与真正的数据读写** 从而达到降低负载的效果，而非 降低与客户端通信的负载。

**无中心节点**  
以 ceph 为代表，每个节点都是自治的、自管理的，整个 ceph 集群只包含一类节点，
如下图 (最下层红色的 RADOS 就是 ceph 定义的“同时包含 meta 数据和文件数据”的节点)：

![ceph架构图](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1596392632776&di=355641ec20b1494bd278d6a63b6eaddc&imgtype=0&src=http%3A%2F%2Fwww.chinastor.com%2Fuploads%2Fallimg%2F160808%2F101I3I56-0.png)

无中心化的最大优点是解决了中心节点自身的瓶颈，这也就是 ceph 号称可以无限向上扩容的原因。但由 Client 直接和 Server 通信，
那么 Client 必须要知道，当对某个文件进行操作时，它该访问集群中的哪个节点。ceph 提供了一个很强大的原创算法来解决这个问题——[CRUSH](https://zhuanlan.zhihu.com/p/59317425) 算法。

## <span id="持久化">持久化</span>
对于文件系统来说，持久化是根本。这主要是通过**多副本**的方式来解决。多副本要考虑的问题有：
- 如何保证每个副本的数据是一致的?
- 如何分散副本，以使灾难发生时，不至于所有副本都被损坏?
- 怎么检测被损坏或数据过期的副本，以及如何处理?
- 该返回哪个副本给 Client?

1. 如何保证每个副本的数据一致  
同步写入是保证副本数据一致的最直接的办法。当 Client 写入一个文件的时候，Server 会等待所有副本都被成功写入，再返回给 Client。  
这种方式简单、有保障，缺陷是性能会受到影响，优化方法有：
- 并行写：由一个副本作为主副本，并行发送数据给其他副本；
- 链式写：几个副本组成一个链 (chain)，并不是等内容都接受到了再往后传播，而是像流一样，边接收上游传递过来的数据，一边传递给下游。
- W+R>N：比如 3 副本 (N=3) 的情况，W＝2，R＝2，即成功写入 2 个就认为成功，读的时候也要从 2 个副本中读（不是 HDFS 的 ISR，完全不同的概念）。
这种方式通过牺牲一定的读成本，来降低写成本，同时增加写入的可用性。这种方式在分布式文件系统中用地比较少。（不得不说，这种方法也是很机智）

2. 如何分散副本，以使灾难发生时，不至于所有副本都被损坏  
为了避免某个机房~~爆炸~~受到灾害而出现数据丢失的情况，要有一个副本应该分配地比较远。
它的副作用是会带来**这个副本的写入性能可能会有一定的下降**，因为它离 Client 最远。所以如果在物理条件上无法保证够用的网络带宽的话，
则读写副本的策略上需要做一定考虑，可以参考同步写入只写 2 副本、较远副本异步写入的方式（两个近的同步写，远的异步写；HDFS 好像不是这样的，它好像都是同步写）；
同时为了保证一致性，读取的时候又要注意一些，避免读取到异步写入副本的过时数据（要优先读近的，HDFS 就这样）。

3. 如何检测被损坏或数据过期的副本，以及如何处理  

分两种情况——有中心节点和无中心节点  

- **有中心节点**：数据节点定期和中心节点进行通信，汇报自己的数据块的相关信息，中心节点将其与自己维护的信息进行对比。
如果某个数据块的 checksum(校验和) 不对，则表明该数据块被损坏了；如果某个数据块的 version 不对，则表明该数据块过期了。
- **无中心节点**：以 ceph 为例，它在自己的节点集群中维护了一个比较小的 monitor(监控) 集群，数据节点向这个 monitor 集群汇报自己的情况，
由其来判定是否被损坏或过期。

4. 该返回哪个副本给 Client
这里的策略就比较多了，比如 round-robin、速度最快的节点、成功率最高的节点、CPU 资源最空闲的节点、甚至就固定选第一个作为主节点，
也可以选择离自己最近的一个（对，就是在说你，HDFS），这样对整体的操作完成时间会有一定节约。

## <span id="伸缩性">伸缩性</span>
**存储节点的伸缩**
当在集群中加入一台新的存储节点，则它主动向中心节点注册，提供自己的信息，当后续有创建文件或者给已有文件增加数据块的时候，
中心节点就可以分配到这台新节点了。但有一些问题需要考虑：
- 如何尽量使各存储节点的负载相对均衡?
- 怎样保证新加入的节点，不会因短期负载压力过大而崩塌?
- 如果需要数据迁移，那如何使其对业务层透明?

1. 如何尽量使各存储节点的负载相对均衡
首先要有评价存储节点**负载的指标**。有多种方式，可以从磁盘空间使用率考虑，也可以从磁盘使用率 +CPU 使用情况 + 网络流量情况等做综合判断。
一般来说，**磁盘使用率是核心指标**。  
其次在分配新空间的时候，优先选择资源使用率小的存储节点；而对已存在的存储节点，如果负载已经过载、或者资源使用情况不均衡，则需要做数据迁移。  
（这个回答似乎没有回答到点子上）  

>**HDFS的负载均衡策略**  
>数据均衡过程的核心是一个数据均衡算法，该数据均衡算法将不断迭代数据均衡逻辑，直至集群内数据均衡为止。该数据均衡算法每次迭代的逻辑如下：
>![数据均衡流程图](https://images.cnblogs.com/cnblogs_com/BYRans/761498/o_balancerLogic.jpg)  
>步骤分析如下：
>1. 数据均衡服务（Rebalancing Server）首先要求 NameNode 生成 DataNode 数据分布分析报告,获取每个 DataNode 磁盘使用情况；
>2. Rebalancing Server 汇总需要移动的数据分布情况，计算具体数据块迁移路线图；数据块迁移路线图，确保网络内最短路径；
>3. 开始数据块迁移任务，Proxy Source Data Node复制一块需要移动数据块；
>4. 将复制的数据块复制到目标DataNode上；
>5. 删除原始数据块；
>6. 目标 DataNode 向 Proxy Source Data Node 确认该数据块迁移完成；
>7. Proxy Source Data Node 向 Rebalancing Server 确认本次数据块迁移完成。然后继续执行这个过程，直至集群达到数据均衡标准。
>
>**DataNode分组**  
>在第2步中，HDFS 会把当前的 DataNode 节点,根据阈值的设定情况划分到 Over、Above、Below、Under 四个组中。
>在移动数据块的时候，Over组、Above组中的块向Below组、Under组移动。

2. 如何保证新加入的节点，不会因短期负载压力过大而崩塌  
当系统发现当前新加入了一台存储节点，显然它的资源使用率是最低的，那么所有的写流量都路由到这台存储节点来，那就可能造成这台新节点短期负载过大。
因此，在资源分配的时候，需要有预热时间，在一个时间段内，缓慢地将写压力路由过来，直到达成新的均衡。

3. 那如何使数据迁移其对业务层透明  
- **有中心节点**：中心节点就包办了——判断哪台存储节点压力较大，判断把哪些文件迁移到何处，更新自己的 meta 信息，
迁移过程中的写入怎么办，发生重命名怎么办，无需上层应用来处理。
- **无中心节点**：代价比较大。举个栗子 ceph，它要采取**逻辑位置和物理位置**两层结构，对 Client 暴露的是逻辑层 (pool 和 place group)，
这个在迁移过程中是不变的，而下层物理层数据块的移动，只是逻辑层所引用的物理块的地址发生了变化，在 Client 看来，逻辑块的位置并不会发生改变。

**中心节点的伸缩**
由于中心节点作为控制中心，是主从模式，那么在伸缩性上就受到比较大的限制——不能超过单台物理机的规模。有几种方式可以尽量地抬高这个上限：
- **以大数据块的形式来存储文件**：它的意义在于大幅减少 metadata 的数量，使中心节点的单机内存就能够支持足够多的磁盘空间 meta 信息。
- **中心节点采取多级的方式**：顶级中心节点只存储目录的 metadata，其指定某目录的文件去哪台次级总控节点去找，
然后再通过该次级总控节点找到文件真正的存储节点；
- **中心节点共享存储设备**：部署多台中心节点，但它们共享同一个存储外设 / 数据库，meta 信息都放在这里，中心节点自身是无状态的。
这种模式下，中心节点的请求处理能力大为增强，但性能会受一定影响（主要应该是通信需要时间和并发情况下会争抢资源）。iRODS 就是采用这种方式。

## <span id="高可用性">高可用性</span>
**中心节点的高可用**  
中心节点的高可用，不仅要保证自身应用的高可用，还得保证 metadata 的高可用。

metadata 的高可用主要是**数据持久化**，并且需要备份机制保证不丢。一般方法是增加一个从节点，主节点的数据实时同步到从节点上。
也有采用共享磁盘，通过 raid1 的硬件资源来保障高可用。

metadata 的数据持久化策略有以下几种方式：
- 直接保存到**存储引擎**上，一般是数据库；
- 直接以文件形式保存到**磁盘**上，也不是不可以，但因为 meta 信息是结构化数据，这样相当于自己研发出一套小型数据库来，复杂化了；
- 保存日志数据到磁盘文件 (类似 MySQL 的 binlog 或 Redis 的 aof)，系统启动时在内存中重建成结果数据，提供服务。
修改时先修改磁盘日志文件，然后更新内存数据。这种方式简单易用。

当前内存服务 + 日志文件持久化是主流方式。一是纯内存操作，效率很高，日志文件的写也是顺序写；二是不依赖外部组件，独立部署。

为了解决日志文件会随着时间增长越来越大的问题，以让系统能以尽快启动和恢复，需要辅助以内存快照的方式——定期将内存 dump 保存，
只保留在 dump 时刻之后的日志文件。这样当恢复时，从最新一次的内存 dump 文件开始，找其对应的 checkpoint 之后的日志文件开始重播。

**存储节点的高可用**
就是**[持久化](#持久化)**，在保证数据副本不丢失的情况下，也就保证了其的高可用性。

## <span id="性能优化和缓存一致性">性能优化和缓存一致性</span>
读写优化：网络传输已经超过了磁盘的能力。网络读写的速度远慢于内存的读写。常见的优化方法主要有：
- 内存中缓存文件内容；
- 预加载数据块，以避免客户端等待；
- 合并读写请求，也就是将单次请求做些积累，以批量方式发送给 Server 端。

缓存的使用在提高读写性能的同时，也会带来数据不一致的问题：
- 会出现更新丢失的现象。当多个 Client 在一个时间段内，先后写入同一个文件时，先写入的 Client 可能会丢失其写入内容，
因为可能会被后写入的 Client 的内容覆盖掉；
- 数据可见性问题。Client 读取的是自己的缓存，在其过期之前，如果别的 Client 更新了文件内容，它是看不到的；也就是说，在同一时间，
不同 Client 读取同一个文件，内容可能不一致。

**貌似跟Java的内存模型差不多？**

这类问题有几种方法：
- **文件只读不改**：一旦文件被 create 了，就只能读不能修改。这样 Client 端的缓存，就不存在不一致的问题；
- **通过锁**：用锁的话还要考虑不同的粒度。写的时候是否允许其他 Client 读? 读的时候是否允许其他 Client 写? 
这是在性能和一致性之间的权衡，作为文件系统来说，由于对业务并没有约束性，所以要做出合理的权衡，比较困难，因此最好是提供不同粒度的锁，
由业务端来选择。但这样的副作用是，业务端的使用成本抬高了。
